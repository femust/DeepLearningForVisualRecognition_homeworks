\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}

\newcommand{\hattheta}{\hat{\theta}}
\newcommand{\hatmintexpect}{\hattheta - \mathbb{E}[\hattheta]}
\newcommand{\expectmintheta}{\mathbb{E}[\hattheta]- \hattheta}
\newcommand{\hatmintexpectsqr}{(\hattheta - \mathbb{E}[\hattheta])^2}
\begin{document}
\section{Bias of an estimator}

We are interested in computing
\begin{equation}
bias(\hat{\sigma}_m^2) = \mathbb{E}[\hat{\sigma}_m^2] - \sigma^2
\end{equation}
We begin by evaluation of $\mathbb{E}[\hat{\sigma}_m^2]$
\begin{equation}
\mathbb{E}[\hat{\sigma}_m^2] = \mathbb{E}[\frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \hat{\mu}_m)] = \frac{(m-1)\sigma^2}{m}
\end{equation}
Because,
\begin{equation}
bias(\hat{\sigma}_m^2) = \frac{(m-1)\sigma^2}{m} - \sigma^{2} \neq 0
\end{equation}
we conclude that the result is biased.
Now let's consider the second component,
\begin{equation}
bias(\hat{\sigma}_m^2) = \frac{1}{m-1}\sum_{i=1}^{m} (x^{(i)} - \hat{\mu}_m)^2
\end{equation}
where $\hat{\mu}_m$ is the sample mean. We are interested in term
\begin{equation}
bias(\hat{\sigma}_m^2) = \mathbb{E}[\hat{\sigma}_m^2] - \sigma^2
\end{equation}
We evaluate
\begin{equation}
 \mathbb{E}[\hat{\sigma}_m^2] =  \mathbb{E}[\frac{1}{m-1}\sum_{i=1}^{m} (x^{(i)} - \hat{\mu}_m)] = \frac{(m-1)\sigma^2}{m-1}
\end{equation}
\begin{equation}
bias(\hat{\sigma}_m^2) = \frac{(m-1)\sigma^2}{m-1} - \sigma^2 = 0
\end{equation}
we conclude that the sample variance is an unbiased estimator.

\section{Bias Variance Trade-off}
Bias is defined as
\begin{equation}
\begin{array}{l}
\mathbb{E}[(\hattheta - \theta)^2] = \\
\mathbb{E}[((\hatmintexpect) + (\expectmintheta))^2] = \\
\mathbb{E}[(\hatmintexpect)^2 + 2(\hatmintexpect)(\expectmintheta) + (\expectmintheta)^2] = \\
\mathbb{E}[(\hatmintexpect)^2] + \mathbb{E}[(2(\hatmintexpect)(\expectmintheta)] + (\expectmintheta)^2] = \\
= Var(\hattheta)+ Bias(\hattheta)^2
\end{array}
\end{equation}
where, $\mathbb{E}(\hatmintexpect)^2 = Var(\hattheta)$, $E[\theta] - \sigma =  Bias(\hattheta)^2$, $\mathbb{E}[\mathbb{E}[\hattheta]]$ and $(\mathbb{E}[\hattheta - \theta) $ is a constant.

\section{MAP for a conditional likelihood}

We have an iid dataset $D = {(X^i,Y^i),i = 1,\dots, m}$. For the distribution $p(X,Y,\theta)$
\begin{equation}
\begin{array}{l}
p(\theta|X,Y)p(Y|X)p(X) = p(Y|X,\theta)p(X|\theta)P(\theta) \\
p(\theta | X,Y) = \frac{p(Y|X,\theta)p(X|\theta)P(\theta)}{p(Y|X)p(X)}
\end{array}
\end{equation}	
where the term $P(X|\theta) = P(X)$ since the observations are independent on $\theta$ and so $P(Y|X)$. Therefore,
\begin{equation}
\begin{array}{l}
\mathbf{\theta_{MAP}} = 
\underset{x}{\mathrm{argmax}}~p(\theta|X,Y) = \\
\underset{x}{\mathrm{argmax}}~p(Y|X,\theta)P(\theta) = \\
\underset{x}{\mathrm{argmax}}~\log p(Y|X,\theta)P(\theta) = \\
\underset{x}{\mathrm{argmax}}~[\log p(Y|X,\theta) + \log P(\theta)] = \\
\underset{x}{\mathrm{argmax}}~[log \prod_{i=1}^{m}p(Y^{(i)} | X^{(i)},\theta) + \log p(\theta)] = \\
\underset{x}{\mathrm{argmax}}~[\sum_{i=1}^{m}\log~p(Y^{(i)} | X^{(i)}, \theta) + \log p(\theta)] 
\end{array}
\end{equation}

\section{Derivatives and the chain rule}
\begin{equation}
f_{W,b}(x_i) = \sigma(W^Tx_i + b)
\end{equation}
\begin{equation}
t = W^Tx_i
\end{equation}
\begin{equation}
\sigma(t) = \frac{1}{1+\exp^{-t}}
\end{equation}
\begin{equation}
\frac{\partial{f}}{\partial{t}} = \frac{\partial{\sigma}}{\partial{t}} = \frac{e^{-t}}{(1+e^-t)^2} = \sigma(T)(1-\sigma(t))
\end{equation}

\begin{equation}
\frac{\partial{L}}{\partial{W_j}} = \frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{t}}\frac{\partial{t}}{\partial{W_j}} = 2\sum_{i=1}^N (y_i - f_i)(-1)(f_i)(1-f_i)(x_{ij})
\end{equation}

\begin{equation}
\frac{\partial{L}}{\partial{b}} = \frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{t}}\frac{\partial{t}}{\partial{b}} = 2\sum_{i=1}^N (y_i - f_i)(-1)(f_i)(1-f_i)(1)
\end{equation}
Can be constructed from partial derivatives above
\begin{equation}
\nabla_WL = [\frac{\partial{L}}{\partial{W_1}} \dots \frac{\partial{L}}{\partial{W_d}}]^T
\end{equation}
for weight to get to the local minimum
\begin{equation}
W \leftarrow W - \alpha \nabla_W L
\end{equation}
similar for bias 
\begin{equation}
b \leftarrow b - \alpha \nabla_b L
\end{equation}
\end{document}


