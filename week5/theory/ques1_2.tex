\documentclass{article}
\usepackage{amsmath,amssymb}

\title{Deep learning Assignment 5}
\author{Aysha Athar Siddiqui\\
Andrzej Reinke\\
Chang Chun Peng\\
Ramaswamy Venkatachalam\\
Yash Goel
}
\date{January 2021}
\begin{document}
\maketitle
\section{Theoretical Task }

\subsection{Mean Square Error \& Estimators}
\subsubsection{}
For our data,

\begin{equation*}
    p(y|x,\theta) = \mathcal{N}(y|f(x;\theta), \sigma^2)
\end{equation*}

So, given i.i.d samples, the log likelihood is:

\begin{align*}
    \text{log } p(y|x,\theta) &= \text{log } \prod_{i} p(y_{i}|x_{i},\theta) = \sum_{i}\text{log }p(y_{i}|f(x;\theta), \sigma^2)\\
    &= \sum_{i}\text{log }\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\Big({-\frac{(y_{i} - f(x;\theta))^{2}}{2\sigma^{2}}}\Big)\\
    &= m\text{log }\frac{1}{\sqrt{2\pi\sigma^{2}}} - \sum_{i}\Big({\frac{(y_{i} - f(x;\theta))^{2}}{2\sigma^{2}}}\Big)
\end{align*}

Neglecting the first time, while maximizing the log likelihood we get,

\begin{align*}
    \max (LL) &= \max - \sum_{i}\Big({\frac{(y_{i} - f(x;\theta))^{2}}{2\sigma^{2}}}\Big)\\
    &= \min \sum_{i}\Big({\frac{(y_{i} - f(x;\theta))^{2}}{2\sigma^{2}}}\Big)\\
    & \cong \min \frac{1}{m}\sum_{i}{||y_{i} - f(x;\theta)||^2}\\
\end{align*}

which is equivalent to minimizing the MSE.

\subsubsection{}

Expanding the posterior over $\theta$ using Bayes theorem and avoiding the constants which don't affect the maximizing we get,

\begin{align*}
    p(\theta|y,x) &\approx p(y|\theta,x)p(\theta)
\end{align*}

which shows posterior is proportional to the likelihood and the prior on weights

now taking log and taking max to get MAP,

\begin{align*}
    MAP &= \max \text{log }p(y|\theta, x) + \max \text{log }p(\theta)\\
    &= max(LL) + \max \text{log }\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\Big({-\frac{\theta^{2}}{2\sigma^{2}}}\Big)\\
    &\cong max(LL) + \max -\frac{1}{2}||\theta||^2\\
    &= \min \Big(\frac{1}{m}\sum_{i}{||y_{i} - f(x;\theta)||^2} + \frac{1}{2}||\theta||^2 \Big)
\end{align*}

which is minimizingg the MSE with l2-regularization.

\subsubsection{}

From previous parts we know,

\begin{align*}
    MAP &= \max \text{log }p(y|\theta, x) + \max \text{log }p(\theta)\\
\end{align*}

So, when the prior on the weights in a uniform distribution then we get,

\begin{align*}
    MAP &= \max \text{log }p(y|\theta, x)\\
\end{align*}

hence, maximum likelihood estimation is equal to maximum a posteriori estimation when the prior on weights is uniform distribution.



\subsection{Cross Entropy Loss \& Label Smoothing}

\subsubsection{}

Our model has a softmax output with cross entropy loss. Which looks like output for each class $i$ with score $s_i$,

\begin{align*}
    f(s)_{i} = \frac{e^{s_{i}}}{\sum_{i}e^{s_{i}}}
\end{align*}

Cross entropy loss applied to it we get loss as, $-\sum_{i}t_{i}log(f(s)_{i})$ where $t_i$ denotes if that sample belongs to that class $i$ depending on the training labels which is like one-got encoded vector.\\

Now, we can see that our softmax output in a distribution which can be characterised with $p(y|x,\theta)$. Also the training distribution over the classes - which is our ground truth can be denoted by $q(y|x)$. The cross entropy error between the distributions gives the same form of our model loss. 

\begin{align*}
    CE(p,q) = -\sum_{y}q(y|x)log(p(y|x,\theta))
\end{align*}

Minimizing this loss is equivalent to bring the distribution closer - in our case the model distribution closer to the ground truth distribution. Hence, this model ensures we learn the ground truth distribution.

\subsubsection{}

So, minimizing the cross entropy loss is:

\begin{align*}
    \min CE(p,q) = \min -\sum_{y}q(y|x)log(p(y|x,\theta))
\end{align*}

taking the sum over all the samples we get,

\begin{align*}
    \min CE(p,q) &= \min -\sum_{i}\sum_{y}q(y|x)log(p(y|x,\theta))\\
    &= \max\sum_{i}\sum_{y}q(y|x)log(p(y|x,\theta))\\
\end{align*}

therefore we can see that minimizing the cross entropy loss is equivalent to maximum log likelihood of our model.

\subsubsection{}

So, in the case of label smoothing we introduce noise to our target distribution and hence it is not one-hot encoded output anymore:
\begin{align*}
    \min CE(p,q_{new}) &= \min -\sum_{i}\sum_{y}q_{new}(y|x)log(p(y|x,\theta))\\
    &= \min \sum_{i}\Big(-(1-\epsilon)\sum_{y}q(y|x)log(p(y|x,\theta)) -\epsilon\sum_{y}u(y|x)log(p(y|x,\theta))\Big)\\
    &= \min \sum_{i}\Big((1-\epsilon)CE(p,q) + \epsilon CE(u,p) \Big)\\
\end{align*}
where $u$ is the noise distribution and $\epsilon$ is the probability.\\

When the model distribution reaches close to the ground truth distribution that part of the loss reaches zero, but since it is away from the noise distribution $u$ hence that error will increase therefore it serves as a regularization. If $u$ is an uniform distribution then it becomes independent of the data.



\subsection{Batch normalization}
\subsubsection{}
\begin{align*}
&	z_1 = w^Tx = w_1x_1 + w_3x_2 + b_1 \\
&	z_2 = w^Tx = x_2x_1 + x_4x_2 + b_1
\end{align*}
For $n=1$, for example
\begin{align*}
& z_1 = 0.1 * 0.1 + 0.3 * 0.4 + 0.3 = 0.43  \\
& z_2 = 0.2 * 0.1 + 0.2 * 0.4 + 0.3 = 0.40
\end{align*}
Softmax then gives
\begin{align*}
& o_1 = \frac{e^{z_1}}{e_{z_1} + e_{z_2}} \\
& o_2 = \frac{e^{z_2}}{e_{z_1} + e_{z_2}}
\end{align*}
For $n=1$, for example
\begin{align*}
& o_1 = \frac{e^{0.43}}{e_{0.43} + e_{0.4}} = 0.5075 \\
& o_2 = \frac{e^{0.4}}{e_{0.43} + e_{0.4}} = 0.4925
\end{align*}
Mean square error for $n=1$:
\begin{align*}
& MSE = \frac{1}{2}( p_1 - o_1)^2 + \frac{1}{2}(p_2 - o_2)^2 \\
& MSE = \frac{1}{2}(0.1 - 0.5075 )^2 + \frac{1}{2}( 0.9 - 0.4925)^2 = 0.1660
\end{align*}
Gradient for MSE for $x_i$
\begin{align*}
\frac{\partial L}{\partial w_{ij}} = (p_i - o_i) \sigma(j)(\delta_{ij} - \sigma(i)) w_i
\end{align*}
So for example for $x_1$ 
\begin{align*}
\frac{\partial L}{\partial w_{12}} = (p_1 - o_1) (\frac{e^{z_1}}{e^{z_1} + e^{z_2}}) (1 - \frac{e^{z_1}}{e^{z_1} + e^{z_2}}) w_1
\end{align*}
and for $x_2$ 
\begin{align*}
\frac{\partial L}{\partial w_{21}} = (p_1 - o_1) (\frac{e^{z_1}}{e^{z_1} + e^{z_2}}) (\frac{-e^{z_2}}{e^{z_1} + e^{z_2}}) w_2
\end{align*}
and for $b$ 
\begin{align*}
\frac{\partial L}{\partial b} = (p_1 - o_1) (\frac{e^{z_1}}{e^{z_1} + e^{z_2}}) (\frac{-e^{z_2}}{e^{z_1} + e^{z_2}})*1
\end{align*}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
		\hline
	n & $x_1$ & $x_2$ & $p_1$ & $p_2$ & L&  $dw_{11}$ & $dw_{21}$ & $dw_{22}$ & $dw_{12}$ \\
	1 & $0.1$ & $0.4$ & $0.1$ & $0.9$ & 0.153&  0.010 & -0.030 & -0.019 & 0.093 \\
	2 & $0.8 $ & $0.2$ & $0.95$ & $0.05$ & 0.208&  -0.012 & 0.035 & 0.023 & -0.111 \\
	3 & $0.6$ & $0.5$ & $0.4$ & $0.6$ & 0.010&  0.002 & -0.007 & -0.005 & 0.026 \\
	4 & $0.3$ & $0.9 $ & $0.75$ & $0.25$ & 0.057&  -0.006 & 0.0177 & 0.012 & -0.061 \\
	5 & $0.3$ & $0.5$ & $0.9$ & $0.1$ & 0.163&  -0.010 & 0.030 & 0.021 & -0.103 \\
		\hline
	\end{tabular}
\end{center}
\subsubsection{}
Batch normalization is done according to formulas
\begin{align*}
& \mu_B = \frac{1}{5} \sum_{i=1}^{m=5}x_i \\
& \sigma_B^2 = \frac{1}{5} \sum_{i=1}^{m=5}(x_i - \mu_B)  \\
& \hat{x} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2}}
\end{align*}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
		\hline
		n & $x_1$ & $x_2$ & $p_1$ & $p_2$ & L&  $dw_{11}$ & $dw_{21}$ & $dw_{22}$ & $dw_{12}$ \\
		1 & $0.1$ & $0.4$ & $0.1$ & $0.9$ & 0.123&  0.011& -0.032 & -0.013 & 0.065 \\
		2 & $0.8 $ & $0.2$ & $0.95$ & $0.05$ & 0.247&  -0.013 & 0.038 & 0.023& -0.116 \\
		3 & $0.6$ & $0.5$ & $0.4$ & $0.6$ & 0.007&  0.002 & -0.006 & -0.005 & 0.023 \\
		4 & $0.3$ & $0.9 $ & $0.75$ & $0.25$ & 0.050&  -0.005 & 0.014 & 0.012 & -0.061 \\
		5 & $0.3$ & $0.5$ & $0.9$ & $0.1$ & 0.189 & -0.010 & 0.029 & 0.024 & -0.119 \\
		\hline
	\end{tabular}
\end{center}

\subsubsection{}
As far as we understood the question the answer is yes. But this normalization will be only locally "for a batch" but not globally for "data" what may influence the learning but it strongly depends on batch data parameters since it may act like a regularization for a deep net with "online learning". On the other hand if we consider batching data we can average the parameters over time at the end.
\end{document}

